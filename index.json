
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hi, I’m Felix. Welcome to my personal website. Here, you can find my latest research, information about my teaching, and open thesis positions. If you are interested in any of my work, or to collaborate, feel free to reach out to me. I am happy to chat with you!\n","date":1693872000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1693872000,"objectID":"b46e398a9b4efa71fa1cf2b80d9293cb","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi, I’m Felix. Welcome to my personal website. Here, you can find my latest research, information about my teaching, and open thesis positions. If you are interested in any of my work, or to collaborate, feel free to reach out to me.","tags":null,"title":"Felix Meissen","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://felix-meissen.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"I am supervising Master’s theses, Guided Reasearch, and IDP at TUM. If you want to work with me, shoot me an E-Mail including your CV, grade transcript and a short motivation (don’t use LLMs for that).\n","date":1696896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696896000,"objectID":"420740fb42f244345ef28c09883ba999","permalink":"https://felix-meissen.github.io/post/initiative_application/","publishdate":"2023-10-10T00:00:00Z","relpermalink":"/post/initiative_application/","section":"post","summary":"I am supervising Master’s theses, Guided Reasearch, and IDP at TUM. If you want to work with me, shoot me an E-Mail including your CV, grade transcript and a short motivation (don’t use LLMs for that).","tags":null,"title":"Initiative application","type":"post"},{"authors":["Philip Müller","Felix Meissen","Georgios Kaissis","Daniel Rückert"],"categories":null,"content":"Getting bounding box labels for pathology detection in chest X-ray images is expensive and time-consuming. Anaomical regions and corresponding texts (visual grounding) on the other hand, can be mined in an automatic fashion, and is, therefore, available at a large scale.\nHow does it work? Fig. 1. Overview: Anatomical regions are first detected using a CNN backbone and a shallow detector. For each region, observed pathologies are predicted using a shared classifier. Bounding boxes for each pathology are then predicted by considering regions with positive predictions and fusing overlapping boxes.\nHow well does it work? Fig. 2. Qualitative results of Loc-ADPD, with predicted (solid) and target (dashed) boxes. Cardiomegaly (red) is detected almost perfectly, as it is always exactly localized at one anatomical region. Other pathologies like atelectasis (blue), effusion (green), or pneumonia (cyan) are detected but often with non-perfect overlapping boxes. Detection also works well for predicting several overlapping pathologies (second from left).\nTab. 1. Results on the NIH ChestX-ray 8 dataset. Our models Loc-ADPD and MIL-ADPD, trained using anatomy (An) bounding boxes, both outperform all weakly supervised methods trained with image-level pathology (Pa) and anatomy-level pathol- ogy (An-Pa) labels by a large margin. MIL-ADPD is competitive with the supervised baseline trained with pathology (Pa) bounding boxes, while Loc-ADPD outperforms it by a large margin.\n","date":1693872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693872000,"objectID":"150478433ee05843271a8fb7dddad064","permalink":"https://felix-meissen.github.io/publication/anatomy_driven/","publishdate":"2023-09-05T00:00:00Z","relpermalink":"/publication/anatomy_driven/","section":"publication","summary":"Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches -- supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.","tags":[],"title":"Anatomy-Driven Pathology Detection on Chest X-rays","type":"publication"},{"authors":["Felix Meissen","Svenja Breuer","Moritz Knolle","Alena Buyx","Ruth Müller","Georgios Kaissis","Benedikt Wiestler","Daniel Rückert"],"categories":null,"content":"Summary Background With the ever-increasing amount of medical imaging data, the demand for algorithms to assist clinicians has amplified. Unsupervised anomaly detection (UAD) models promise to aid in the crucial first step of disease detection. While previous studies have thoroughly explored fairness in supervised models in healthcare, for UAD, this has so far been unexplored.\nMethods In this study, we evaluated how dataset composition regarding subgroups manifests in disparate performance of UAD models along multiple protected variables on three large-scale publicly available chest X-ray datasets. Our experiments were validated using two state-of-the-art UAD models for medical images. Finally, we introduced a novel subgroup-AUROC (sAUROC) metric, which aids in quantifying fairness in machine learning.\nFindings Our experiments revealed empirical “fairness laws” (similar to “scaling laws” for Transformers) for training-dataset composition: Linear relationships between anomaly detection performance within a subpopulation and its representation in the training data. Our study further revealed performance disparities, even in the case of balanced training data, and compound effects that exacerbate the drop in performance for subjects associated with multiple adversely affected groups.\nInterpretation Our study quantified the disparate performance of UAD models against certain demographic subgroups. Importantly, we showed that this unfairness cannot be mitigated by balanced representation alone. Instead, the representation of some subgroups seems harder to learn by UAD models than that of others. The empirical fairness laws discovered in our study make disparate performance in UAD models easier to estimate and aid in determining the most desirable dataset composition.\nResults Fig. 3: a) A linear relationship between the representation of a subgroup in the training dataset and its performance was observed across all datasets and subgroups. Equal representation of subgroups did not produce the most group-fair results. Experimental results for the FAE on the MIMIC-CXR, CXR14, and CheXpert datasets trained under different gender, age, or race imbalance ratios. Each box extends from the lower to upper quartile values of ten runs with different random seeds with a line at the median. Regression lines along the different imbalance ratios are additionally plotted. The exact numbers can be found in the Appendix. b) The mean absolute errors (MAE) between the real subgroup performances and those estimated using the “fairness laws” for each dataset and protected variable. Each box again shows the results over ten runs with different random seeds.\n","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"a1ad4bca22cec1ee4a1afe1918e40519","permalink":"https://felix-meissen.github.io/publication/fairness/","publishdate":"2023-10-06T00:00:00Z","relpermalink":"/publication/fairness/","section":"publication","summary":"We discovered that performance bias between demographic subgroups in unsupervised anomaly detection follows certain patterns, making it highly predictable.","tags":["Source Themes"],"title":"(Predictable) Performance Bias in Unsupervised Anomaly Detection","type":"publication"},{"authors":["Ioannis Lagogiannis","Felix Meissen","Georgios Kaissis","Daniel Rückert"],"categories":null,"content":"Results We compared 13 different anomaly detection models from 4 categories (Image Reconstruction (IR), Feature-modeling (FM), Attention-based (AB), and self-supervised (S-S)) on four different real-world medical datasets. Feature modeling (FM) methods outperform other consistently in almost all datasets and metrics considered.\nVisual Tabular ","date":1690502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690502400,"objectID":"4e20b95635540ca0938209ceeb59b4dd","permalink":"https://felix-meissen.github.io/publication/deep_dive/","publishdate":"2023-07-28T00:00:00Z","relpermalink":"/publication/deep_dive/","section":"publication","summary":"So far the largest comparison of anomaly detection models with various paradigms on four medical imaging datasets.","tags":["Source Themes"],"title":"Unsupervised Pathology Detection: A Deep Dive Into the State of the Art","type":"publication"},{"authors":["Felix Meissen","Philip Müller","Georgios Kaissis","Daniel Rückert"],"categories":null,"content":"Find out more Installation RoDeO is available as a python package for python 3.7+ as rodeometric. To install, simply install it with pip:\npython -m pip install rodeometric Usage import numpy as np from rodeo import RoDeO # Init RoDeO with two classes rodeo = RoDeO(class_names=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]) # Add some predictions and targets pred = [np.array([[0.1, 0.1, 0.2, 0.1, 0.0], [0.0, 0.3, 0.1, 0.1, 1.0], [0.2, 0.2, 0.1, 0.1, 0.0]])] target = [np.array([[0.0, 0.0, 0.1, 0.1, 0.0], [0.0, 0.2, 0.1, 0.1, 1.0]])] rodeo.add(pred, target) # Compute the score score = rodeo.compute() for key, val in score.items(): print(f\u0026#39;{key}: {val}\u0026#39;) ","date":1680652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680652800,"objectID":"124b1f44b5b41adfa2a8a0e49cbff842","permalink":"https://felix-meissen.github.io/publication/rodeo/","publishdate":"2023-04-05T00:00:00Z","relpermalink":"/publication/rodeo/","section":"publication","summary":"RoDeO is an easy to use object detection metric. It evaluates three sources of errors (misclassification, faulty localization, and shape mismatch) separately and combines them to one score.","tags":[],"title":"Robust Detection Outcome: A Metric for Pathology Detection in Medical Images","type":"publication"},{"authors":["Florian Kofler","Johannes Wahle","Ivan Ezhov","Sophia Wagner","Rami Al-Maskari","Emilia Gryska","Mihail Todorov","Christina Bukas","Felix Meissen","Tingying Peng","Ali Ertürk","Daniel Rueckert","Rolf Heckemann","Jan Kirschke","Claus Zimmer","Benedikt Wiestler","Bjoern Menze","Marie Piraud"],"categories":null,"content":"","date":1672444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672444800,"objectID":"17194b7b285deea6873622a020cad522","permalink":"https://felix-meissen.github.io/publication/peak_gt/","publishdate":"2022-12-31T00:00:00Z","relpermalink":"/publication/peak_gt/","section":"publication","summary":"Machine learning models are typically evaluated by computing similarity with reference annotations and trained by maximizing similarity with such. Especially in the biomedical domain, annotations are subjective and suffer from low inter- and intra-rater reliability. Since annotations only reflect one interpretation of the real world, this can lead to sub-optimal predictions even though the model achieves high similarity scores. Here, the theoretical concept of PGT is introduced. PGT marks the point beyond which an increase in similarity with the \\emph{reference annotation} stops translating to better RWMP. Additionally, a quantitative technique to approximate PGT by computing inter- and intra-rater reliability is proposed. Finally, four categories of PGT-aware strategies to evaluate and improve model performance are reviewed.","tags":[],"title":"Approaching Peak Ground Truth","type":"publication"},{"authors":null,"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"b7b6a8241b470c24f9c59de74dce7b97","permalink":"https://felix-meissen.github.io/project/seminar_anomaly_detection/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/project/seminar_anomaly_detection/","section":"project","summary":"Master-Seminar at TUM.","tags":["Demo"],"title":"Unsupervised Anomaly Detection in Medical Imaging","type":"project"},{"authors":["Felix Meissen","Johannes Pätzold","Georgios Kaissis","Daniel Rückert"],"categories":null,"content":"Overview Our strongest model so far 💪. Anomaly detection with image-reconstruction models doesn’t work well if anomalies are not hyperintense. FAE solves this problem.\nFig. 1. FAE trains an autoencoder not in image-space, but in the feature-space of a pretrained ResNet. This allows it to also capture anomalies that are not hyperintense in image-space. Using the Structural Similarity Index Measure (SSIM) further helps with this problem.\nResults Our model outperforms all competitors by a large margin. A recent analysis of 2023 still found it the strongest anomaly detection model so far.\nFig. 2. Performance of all compared models and a random classifier, including error bars that indicate one standard deviation over $N = 5$ runs with different random seeds, on the BraTS data set. Our method performs statistically significantly better than all compared methods (t-test; $p ≤ 0.05$).\nFig. 3. Examples of successful and failure cases.\n","date":1661212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661212800,"objectID":"c90597461bbe56fe4bfaf0a9b09bded6","permalink":"https://felix-meissen.github.io/publication/fae/","publishdate":"2022-08-23T00:00:00Z","relpermalink":"/publication/fae/","section":"publication","summary":"Our strongest model so far. Anomaly detection with image-reconstruction models doesn't work well if anomalies are not hyperintense. FAE solves this problem.","tags":[],"title":"Unsupervised Anomaly Localization with Structural Feature-Autoencoders","type":"publication"},{"authors":["Felix Meissen","Ioannis Lagogiannis","Georgios Kaissis","Daniel Rückert"],"categories":null,"content":"Unsupervised anomaly detection models can distinguish samples from the training distribution from samples from other distributions. Many works have benchmarked their models on the Hyper-Kvasir dataset: A dataset of gastrointestinal endoscopy videos to detect polyps in them. We show that anomaly detection models trained on this dataset actually detect general distribution shifts instead of polyps.\nIn normal images, the operator simply navigates the endoscopy through the gastrointernal tract. If they see a polyp, they will inspect it more closely, resulting in vastly different orientations, and lighting conditions. These two distributions are greatly different, but the presence of a polyp is not the largest difference between them.\nFig. 1. Normal training and abnormal test images in Hyper-Kvasir come from different distributions.\n","date":1652054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652054400,"objectID":"b2d7eef943305c5c1b2edc6e87729439","permalink":"https://felix-meissen.github.io/publication/confounding_hyper_kvasir/","publishdate":"2022-05-09T00:00:00Z","relpermalink":"/publication/confounding_hyper_kvasir/","section":"publication","summary":"We show that anomaly detection models trained to detect polyps in the Hyper-Kvasir dataset instead only detect a general domain shift in the images.","tags":[],"title":"Domain Shift as a Confounding Variable in Unsupervised Pathology Detection","type":"publication"},{"authors":["Felix Meissen","Benedikt Wiestler","Georgios Kaissis","Daniel Rückert"],"categories":null,"content":"In this work, we experimentally showed that image-reconstruction based anomaly detection models can not detect anomalies that are not characterized by hypo- or hyper-intensity.\nExperiments Fig. 1. If the reconstruction of an image is only slightly imperfect (simulated with very light gaussian blurring), detection performance for anomalies with intensities between 0.2 and 0.8 decreases dramatically.\nFig. 2. Our experiments from Fig. 1 are confirmed by experiments with real anomaly detection models that also fail to detect anomalies that are not hyperintense.\nFig. 3. This figure shows the capacity-tradeoff image-reconstruction models face. If the capacity is too low, reconstruction will be imperfect (AE, first two columns). If the capacity is too high, the model will simply reconstruct the anomalies as well (VQ-VAE, third column). Currently, there is no sweet-spot for this problem.\n","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"10ce4bafc6336dac72e7718553acc100","permalink":"https://felix-meissen.github.io/publication/pitfalls/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/publication/pitfalls/","section":"publication","summary":"In this work, we experimentally showed that image-reconstruction based anomaly detection models can not detect anomalies that are not characterized by hypo- or hyper-intensity.","tags":[],"title":"On the Pitfalls of Using the Residual as Anomaly Score","type":"publication"},{"authors":["Felix Meissen","Georgios Kaissis","Daniel Rückert"],"categories":null,"content":"Results Fig. 1. Qualitative results of our baseline. Two samples are shown for each data set. Top row: input image. Middle row: predicted anomaly map, binarized using the thresh- old that yields the best DSC for each data set. Bottom row: ground truth anomaly segmentation. Tab. 1. Comparison of thresholding to current models\nMSLUB MSSEG2015 Method ⌈DSC⌉ AUPRC AUROC ⌈DSC⌉ AUPRC AUROC AE (dense) 0.271 0.163 0.794 0.185 0.080 0.879 AE (spatial) 0.154 0.065 0.732 0.106 0.037 0.781 VAE (restauration) 0.333 0.275 0.839 0.272 0.202 0.905 GMVAE (restauration) 0.332 0.271 0.836 0.280 0.199 0.909 f-AnoGAN 0.283 0.221 0.856 0.342 0.255 0.923 SSAE (spatial) 0.301 0.222 - - - - Ours 0.374 0.271 0.991 0.431 0.262 0.996 Tab. 2. Comparison of thresholding to an ensemble of 8 vision transformers\n⌈DSC⌉ Method BraTS MSLUB WMH Transformer 0.759 0.465 0.441 Ours 0.738 0.613 0.557 ","date":1631491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631491200,"objectID":"cb6021c78866539e3aef8ebc2d1cf61d","permalink":"https://felix-meissen.github.io/publication/challenging/","publishdate":"2021-09-13T00:00:00Z","relpermalink":"/publication/challenging/","section":"publication","summary":"All current anomaly detection models are outperformed via simple thresholding on the commonly used modality of FLAIR MRI.","tags":[],"title":"Challenging Current Semi-supervised Anomaly Segmentation Methods for Brain MRI","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://felix-meissen.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]